{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version is 2.3.0+cu121\n",
      "is CUDA available : True\n",
      "number of available threads : 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version is {torch.__version__}\")\n",
    "print(f\"is CUDA available : {torch.cuda.is_available()}\")\n",
    "print(f\"number of available threads : {torch.get_num_threads()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Roberts Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import numpy as np \n",
    "from scipy import ndimage \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_preprocessing_machine(img):\n",
    "    transform = torchvision.transforms.Resize(size=(810,1080))\n",
    "    img = transform(img)\n",
    "    transforms = v2.Compose([\n",
    "    #v2.RandomHorizontalFlip(p=0.5),\n",
    "    #v2.RandomVerticalFlip(p=0.5),\n",
    "    #v2.functional.autocontrast(0.5),\n",
    "    v2.ToDtype(torch.float64, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.ColorJitter(brightness=1,contrast=0.5,saturation=0.5),\n",
    "    #v2.CenterCrop(size=(810, 810)),\n",
    "    #v2.RandomSolarize(threshold=0.9),\n",
    "    ])\n",
    "    print(img.shape)\n",
    "    img2 = transforms(img)\n",
    "    img2 = v2.functional.to_grayscale(img2)\n",
    "    #sharpen = v2.functional.adjust_sharpness(img2,sharpness_factor=1.0)\n",
    "    adj_con = v2.functional.adjust_contrast(inpt=img2, contrast_factor = 1)\n",
    "    adj_bri = v2.functional.adjust_brightness(inpt= adj_con, brightness_factor = 1)\n",
    "    adj_sat = v2.functional.adjust_saturation(inpt= adj_bri, saturation_factor = 0.5)\n",
    "    plt.imshow(img2.permute(1, 2, 0), cmap=\"gray\",aspect=1)\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_robert_img(img_path):\n",
    "    #print(img_path)\n",
    "    path = img_path.split(\"\\\\\")[-1]\n",
    "    #print(path)\n",
    "    roberts_cross_v = np.array( [[1, 0 ], \n",
    "                             [0,-1 ]] ) \n",
    "  \n",
    "    roberts_cross_h = np.array( [[ 0, 1 ], \n",
    "                                [ -1, 0 ]] ) \n",
    "    \n",
    "    img = cv2.imread(img_path,0).astype('float64') \n",
    "    img/=255.0\n",
    "    #print(img.shape)\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    vertical = ndimage.convolve( img, roberts_cross_v ) \n",
    "    horizontal = ndimage.convolve( img, roberts_cross_h ) \n",
    "    #plt.imshow(img,cmap=\"gray\")\n",
    "    \n",
    "    edged_img = np.sqrt( np.square(horizontal) + np.square(vertical)) \n",
    "    edged_img*=255\n",
    "    path_save = r\"dataset_peak_1\\test\"\n",
    "    save_name = path\n",
    "    path_save2 = os.path.join(path_save,save_name)\n",
    "    #print(path_save2)\n",
    "    #plt.imshow(edged_img,cmap='gray')\n",
    "    cv2.imwrite(path_save2,edged_img)\n",
    "\n",
    "#cv2.imwrite(\"output192.jpg\",gen_robert_img(path_class_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_os = r\"liver-ultrasound-detection\\test\\test\\images\"\n",
    "img_list = os.listdir(r\"liver-ultrasound-detection\\test\\test\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_images(img_list, path_os):\n",
    "    for i in tqdm(img_list):\n",
    "        path1 = os.path.join(path_os,i)\n",
    "        x = gen_robert_img(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5153/5153 [32:27<00:00,  2.65it/s]\n"
     ]
    }
   ],
   "source": [
    "#gen_images(img_list, path_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import functional\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2_list = os.listdir(r\"dataset_peak_1\\test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(img_path):\n",
    "    img = read_image(img_path)\n",
    "    name = img_path.split(os.sep)\n",
    "    #print(name)\n",
    "    adj_con = v2.functional.adjust_contrast(inpt=img, contrast_factor = 10)\n",
    "    adj_bri = v2.functional.adjust_brightness(inpt= adj_con, brightness_factor = 10)\n",
    "    adj_sat = v2.functional.adjust_saturation(inpt= adj_bri, saturation_factor = 10)\n",
    "    x = functional.to_pil_image(adj_sat)\n",
    "    save_path = os.path.join(r\"Final_dataset_peak\\test\",name[-1])\n",
    "    #print(save_path)\n",
    "    x.convert('RGB')\n",
    "    #x.show()\n",
    "    x.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_preprocessing(path):\n",
    "    img = read_image(path)\n",
    "    transform = torchvision.transforms.Resize(size=(810,1080))\n",
    "    img = transform(img)\n",
    "    img3 = img/255\n",
    "    mean = img3.reshape(3,-1).mean(axis=1)\n",
    "    std  =  img3.reshape(3,-1).std(axis=1)\n",
    "    transforms = v2.Compose([\n",
    "    #v2.RandomHorizontalFlip(p=0.5),\n",
    "    #v2.RandomVerticalFlip(p=0.5),\n",
    "    #v2.functional.autocontrast(0.5),\n",
    "    v2.ToDtype(torch.float64, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std),\n",
    "    #v2.ColorJitter(brightness=1,contrast=0.5,saturation=0.5),\n",
    "    #v2.CenterCrop(size=(810, 810)),\n",
    "    #v2.RandomSolarize(threshold=0.9),\n",
    "    ])\n",
    "    print(img.shape)\n",
    "    img2 = transforms(img)\n",
    "    img2 = v2.functional.to_grayscale(img2)\n",
    "    #sharpen = v2.functional.adjust_sharpness(img2,sharpness_factor=1.0)\n",
    "    img3 = torchvision.transforms.functional.invert(img2)\n",
    "    adj_con = v2.functional.adjust_contrast(inpt=img3, contrast_factor = 0.65)\n",
    "    adj_bri = v2.functional.adjust_brightness(inpt= img2, brightness_factor = 0.8)\n",
    "    adj_sat = v2.functional.adjust_saturation(inpt= adj_bri, saturation_factor = 0.5)\n",
    "    plt.imshow(adj_bri.permute(1, 2, 0), cmap=\"gray\",aspect=1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation(r\"dataset_peak_1\\test\\195.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation(r\"dataset_peak_1\\test\\1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r\"dataset_peak_1\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_images2(img_path, img_list):\n",
    "    for i in tqdm(img_list):\n",
    "        path = os.path.join(img_path, i)\n",
    "        augmentation(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5153/5153 [09:57<00:00,  8.62it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_images2(img_path,aug2_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Roberts Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_preprocessing_mobile(img):\n",
    "    transform = torchvision.transforms.Resize(size=(810,1080))\n",
    "    img = transform(img)\n",
    "    transforms = v2.Compose([\n",
    "    #v2.RandomHorizontalFlip(p=0.5),\n",
    "    #v2.RandomVerticalFlip(p=0.5),\n",
    "    #v2.functional.autocontrast(0.5),\n",
    "    v2.ToDtype(torch.float64, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.ColorJitter(brightness=1,contrast=0.5,saturation=0.5),\n",
    "    #v2.CenterCrop(size=(810, 810)),\n",
    "    #v2.RandomSolarize(threshold=0.9),\n",
    "    ])\n",
    "    print(img.shape)\n",
    "    img2 = transforms(img)\n",
    "    img2 = v2.functional.to_grayscale(img2)\n",
    "    #sharpen = v2.functional.adjust_sharpness(img2,sharpness_factor=1.0)\n",
    "    adj_con = v2.functional.adjust_contrast(inpt=img2, contrast_factor = 1)\n",
    "    adj_bri = v2.functional.adjust_brightness(inpt= adj_con, brightness_factor = 1)\n",
    "    adj_sat = v2.functional.adjust_saturation(inpt= adj_bri, saturation_factor = 0.5)\n",
    "    plt.imshow(img2.permute(1, 2, 0), cmap=\"gray\",aspect=1)\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_os = r\"sample_liver-ultrasound\\train\\images\"\n",
    "img_list = os.listdir(r\"sample_liver-ultrasound\\train\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_images(img_list, path_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2_list = os.listdir(r\"augmented_dataset_peak\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_mobile(img_path):\n",
    "    img = read_image(img_path)\n",
    "    name = img_path.split(os.sep)\n",
    "    adj_con = v2.functional.adjust_contrast(inpt=img, contrast_factor = 100)\n",
    "    adj_bri = v2.functional.adjust_brightness(inpt= adj_con, brightness_factor = 20)\n",
    "    adj_sat = v2.functional.adjust_saturation(inpt= adj_bri, saturation_factor = 100)\n",
    "    x = functional.to_pil_image(adj_sat)\n",
    "    save_path = os.path.join(r\"final_aug\\train\",name[2])\n",
    "    #print(save_path)\n",
    "    x.convert('RGB')\n",
    "    x.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_images2(img_path, img_list):\n",
    "    for i in tqdm(img_list):\n",
    "        path = os.path.join(img_path, i)\n",
    "        augmentation(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_images2(img_path,aug2_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peaktorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
